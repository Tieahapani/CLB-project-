# -*- coding: utf-8 -*-
"""Celebrity_face_classification.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1e_7HVBkzX_9O11nsNRuBekN1vKVAcp8i
"""

from google.colab import files
uploaded = files.upload()

import zipfile
zip_file = "/content/archive (22) (1).zip" 

with zipfile.ZipFile(zip_file, 'r') as zip_ref:
    zip_ref.extractall("/content/dataset")  # Extract to a folder inside Colab

print("Dataset extracted successfully!")

import cv2
 # Import cv2_imshow from google.colab.patches
from google.colab.patches import cv2_imshow

# Path to the correct image file (update with actual path from previous steps)
image_path = "/content/dataset/Celebrity Faces Dataset/Hugh Jackman/049_beaf3777.jpg"  # Adjust if necessary

# Read the image
img = cv2.imread(image_path)

cv2_imshow(img) # Use the imported cv2_imshow function to display the image

import os
import cv2


# Path to the dataset
dataset_path = "/content/dataset/Celebrity Faces Dataset"

all_images = []  # List to store all processed images
X, y = [], []
# Loop through all celebrity folders
for celebrity in os.listdir(dataset_path):
    celebrity_folder = os.path.join(dataset_path, celebrity)

    # Check if it's a folder (not a file)
    if os.path.isdir(celebrity_folder):
        print(f"Celebrity: {celebrity}")

        # Loop through all images in the celebrity folder
        for image_file in os.listdir(celebrity_folder):
            image_path = os.path.join(celebrity_folder, image_file)
            img = cv2.imread(image_path)

            # Process and store the image

            img = cv2.resize(img, (224, 224))
            # Append data to X and y
            X.append(img)
            y.append(celebrity)
            all_images.append(img)  # Add processed image to the list
            print(f"   Image: {image_file}")


# Now you can display specific images using indexing:
cv2_imshow(all_images[0])  # Display the first image
cv2_imshow(all_images[1])  # Display the second image
# ...and so on

actual = cv2_imshow(all_images[500])

import numpy as np

X = np.array(X)
y = np.array(y)

from sklearn.preprocessing import LabelEncoder
le = LabelEncoder()
y = le.fit_transform(y)

from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(X,y, random_state = 42)

X_train_scaled = X_train/ 255
X_test_scaled = X_test/ 255

X_train_scaled[25]

X_train.shape

X_train_scaled.shape

y_train.shape

X_test_scaled

import tensorflow as tf
from tensorflow import keras





ANN = keras.models.Sequential([
    keras.layers.Flatten(input_shape = (224,224,3)), # Changed input_shape to (224, 224, 3)
    keras.layers.Dense(300, activation = "relu"),
    keras.layers.Dense(100, activation = "relu"),
    keras.layers.Dense(17, activation = "softmax")
])

ANN.compile(optimizer='adam',loss='sparse_categorical_crossentropy',metrics=['accuracy'])

ANN.fit(X_train_scaled, y_train, epochs = 50)

ANN.evaluate(X_test_scaled, y_test)

train_loss, train_accuracy = ANN.evaluate(X_train_scaled, y_train)
print(f"Training Loss: {train_loss:.4f}")
print(f"Training Accuracy: {train_accuracy:.4f}")

test_loss, test_accuracy = ANN.evaluate(X_test_scaled, y_test)
print(f"Test Loss: {test_loss:.4f}")
print(f"Test Accuracy: {test_accuracy:.4f}")

import tensorflow as tf
from tensorflow.keras import datasets, layers, models
import numpy as np

cnn = models.Sequential([
    layers.Conv2D(filters=32, kernel_size=(3, 3), activation="relu", input_shape=(224, 224, 3)), # Changed input_shape to (224, 224, 3)
    layers.MaxPooling2D(2, 2),
    layers.Conv2D(filters=64, kernel_size=(3, 3), activation="relu"),
    layers.MaxPooling2D(2, 2),
    layers.Flatten(),
    layers.Dense(150, activation="relu"),
    layers.Dense(17, activation="softmax")
])

cnn.compile(optimizer="adam",
             loss="sparse_categorical_crossentropy",
             metrics=["accuracy"]
             )

cnn.fit(X_train_scaled, y_train, epochs = 50)

cnn.evaluate(X_test_scaled, y_test)

predictions = cnn.predict(X_test_scaled)

predictions[0]

predicted_class = np.argmax(predictions[0])

cv2_imshow(all_images[500])

predicted_class_label = le.inverse_transform([predicted_class])
print(f"Predicted class for the 500th image : {predicted_class_label}")

from sklearn.metrics import classification_report
y_pred = np.argmax(predictions, axis = 1)
print(classification_report(y_test, y_pred))

!pip install tensorflow
import tensorflow as tf
from tensorflow import keras
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, Flatten
from tensorflow.keras.optimizers import Adam
from tensorflow.keras import regularizers
from tensorflow.keras.callbacks import EarlyStopping
from tensorflow.keras.preprocessing.image import ImageDataGenerator
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.metrics import confusion_matrix
from sklearn.model_selection import train_test_split

# -----------------------------
# üöÄ Step 1: Split Data
# -----------------------------
# Assuming X and y are your image arrays and labels
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# -----------------------------
# üßº Step 2: Normalize the data
# -----------------------------
X_train_scaled = X_train / 255.0
X_test_scaled = X_test / 255.0

# -----------------------------
# üîÅ Step 3: Image Data Generator
# -----------------------------
train_datagen = ImageDataGenerator(
    rotation_range=10,
    zoom_range=0.1,
    width_shift_range=0.1,
    height_shift_range=0.1,
    horizontal_flip=True,
    validation_split=0.2  # 20% for validation
)

train_generator = train_datagen.flow(
    X_train_scaled, y_train,
    batch_size=32,
    subset='training',
    shuffle=True
)

val_generator = train_datagen.flow(
    X_train_scaled, y_train,
    batch_size=32,
    subset='validation',
    shuffle=True
)

# -----------------------------
# üì¶ Step 4: Load Pretrained VGG16
# -----------------------------
vgg16_model = keras.applications.vgg16.VGG16(weights='imagenet', include_top=False, input_shape=(224, 224, 3))

# Freeze lower layers
for layer in vgg16_model.layers[:-4]:
    layer.trainable = False

for layer in vgg16_model.layers[-4:]:
    layer.trainable = True

# -----------------------------
# üß† Step 5: Build Model
# -----------------------------
model = Sequential()
model.add(vgg16_model)
model.add(Flatten())
model.add(Dense(400, activation='relu', kernel_regularizer=regularizers.l2(0.01)))
model.add(Dense(500, activation='relu', kernel_regularizer=regularizers.l2(0.01)))
model.add(Dense(256, activation='relu', kernel_regularizer=regularizers.l2(0.01)))
model.add(Dense(17, activation='softmax'))  # 17 celebrity classes

# -----------------------------
# ‚öôÔ∏è Step 6: Compile Model
# -----------------------------
model.compile(
    optimizer=Adam(learning_rate=0.0001),
    loss='sparse_categorical_crossentropy',
    metrics=['accuracy']
)

# -----------------------------
# ‚è±Ô∏è Step 7: Add EarlyStopping
# -----------------------------
early_stop = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)

# -----------------------------
# üèãÔ∏è‚Äç‚ôÇÔ∏è Step 8: Train the Model
# -----------------------------
model.fit(
    train_generator,
    epochs=50,
    validation_data=val_generator,
    callbacks=[early_stop]
)

# -----------------------------
# üß™ Step 9: Evaluate Model
# -----------------------------
test_loss, test_accuracy = model.evaluate(X_test_scaled, y_test)
print(f"Test Loss: {test_loss:.4f}")
print(f"Test Accuracy: {test_accuracy:.4f}")

# -----------------------------
# üîç Step 10: Confusion Matrix
# -----------------------------
y_pred = np.argmax(model.predict(X_test_scaled), axis=1)
conf_mat = confusion_matrix(y_test, y_pred)

plt.figure(figsize=(12, 10))
sns.heatmap(conf_mat, annot=True, fmt='d', cmap='Blues')
plt.xlabel("Predicted")
plt.ylabel("Actual")
plt.title("Celebrity Confusion Matrix")
plt.show()

import tensorflow as tf
from tensorflow import keras
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, Flatten
from tensorflow.keras.optimizers import Adam
from tensorflow.keras import regularizers
from tensorflow.keras.callbacks import EarlyStopping
from tensorflow.keras.preprocessing.image import ImageDataGenerator
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.metrics import confusion_matrix
from sklearn.model_selection import train_test_split

X_train, X_test, y_train, y_test = train_test_split(X,y, test_size = 0.2, random_state = 42)

X_train_scaled = X_train/255
X_test_scaled = X_test/255

import matplotlib.pyplot as plt

# Assuming test_loss and test_accuracy are calculated in a previous cell
test_accuracies = [test_accuracy]  # Create a list containing the test accuracy
test_losses = [test_loss]        # Create a list containing the test loss

plt.figure(figsize=(12, 4))
plt.plot(test_accuracies, marker='o', label='Test Accuracy')
plt.title('Test Accuracy')
plt.xlabel('epoch')
plt.ylabel('Accuracy')
plt.grid(True)
plt.legend()
plt.show()

plt.figure(figsize=(12, 4))
plt.plot(test_losses, marker='o', color='red', label='Test Loss')
plt.title('Test Loss')
plt.xlabel('Model Checkpoint')
plt.ylabel('Loss')
plt.grid(True)
plt.legend()
plt.show()

predictions = model.predict(X_test_scaled)
predicted_labels = np.argmax(predictions, axis = 1)
predicted_labels

predicted_celebrity_names = le.inverse_transform(predicted_labels)
predicted_celebrity_names

true_celebrity_names = le.inverse_transform(y_test)
true_celebrity_names

predicted_celebrity_names[0]
cv2_imshow(all_images[0])

true_celebrity_names[0]
cv2_imshow(all_images[0])

history = model.evaluate(X_test_scaled, y_test)  # Remove epochs parameter
print(f"Test Loss: {history[0]:.4f}")  # Access loss from history[0]
print(f"Test Accuracy: {history[1]:.4f}")  # Access accuracy from history[1]

print(test_loss)

print(test_accuracy)

import numpy as np
from sklearn.preprocessing import LabelEncoder

# Assuming you already have a LabelEncoder instance named 'le'
# and your true labels are in y_train or y_test

# Get unique encoded labels
unique_encoded_labels = np.unique(y_train)  # Or use y_test

# Get the corresponding celebrity names
true_celebrity_names = le.inverse_transform(unique_encoded_labels)

# Print the true celebrity names
print("True celebrity names in the dataset:")
for name in true_celebrity_names:
    print(name)

from google.colab import files
uploaded = files.upload()

import os
os.listdir()

img_path =  "24-18.jpg"

from tensorflow.keras.preprocessing import image
from tensorflow.keras.applications.vgg16 import preprocess_input
import numpy as np
import matplotlib.pyplot as plt

# Step 1: Load and preprocess the image
img_path =  "24-18.jpg"  # Your image path
img = image.load_img(img_path, target_size=(224, 224))  # Resize image to 224x224

# Convert the image to a numpy array
img_array = image.img_to_array(img)

# Expand dimensions to match the model input (batch size, height, width, channels)
img_array = np.expand_dims(img_array, axis=0)

# Preprocess the image (important step for VGG16)
img_array = preprocess_input(img_array)

# Step 2: Predict using your trained model
predictions = model.predict(img_array)

# Get the predicted class index (highest probability)
predicted_class_index = np.argmax(predictions)

# Step 3: Map predicted class index to the corresponding class label
class_names = ['Angelina Jolie', 'Brad Pitt', 'Denzel Washington', 'Hugh Jackman', 'Jennifer Lawrence','Johnny Depp ', 'Kate Winslet', 'Leonardo Dicaprio', 'Megan Fox', 'Natalie Portman', 'Nicole Kidman', 'Robert Downey Jr', 'Sandra Bullock', 'Scarlett Johansson', 'Tom Cruise', 'Tom Hanks', 'Will Smith']  # Add your class names here
predicted_label = class_names[predicted_class_index]

# Step 4: Display the image and predicted label
plt.imshow(img)
plt.title(f"Prediction: {predicted_label}")
plt.axis('off')
plt.show()

# Output predicted label
print(f"Predicted Label: {predicted_label}")

